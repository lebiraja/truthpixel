# AuthentiScan Dataset Guide

This guide details the three datasets used in AuthentiScan, how to download them, and how they are organized.

## Overview

AuthentiScan uses a diverse collection of ~660,000 images totaling ~18GB to ensure robust detection across different generators and domains.

| Dataset | Images | Generators | Size | Purpose |
|---------|--------|------------|------|---------|
| **GenImage** | 400,000 | 8 (SD v1.4/1.5, Midjourney, GLIDE, ADM, VQDM, BigGAN, Wukong) | ~8GB | Multi-generator diversity |
| **CIFAKE** | 120,000 | Stable Diffusion v1.4 | ~3GB | General object detection |
| **140K Faces** | 140,000 | StyleGAN | ~7GB | Deepfake face detection |
| **Total** | **660,000** | **9+ generators** | **~18GB** | **Comprehensive training** |

---

## 1. GenImage Dataset

**Source**: [GenImage Benchmark](https://github.com/GenImage/GenImage)
**Size**: ~8GB
**Images**: 400,000 (200K Real, 200K Fake)

### Description
GenImage is a large-scale benchmark for detecting AI-generated images. It includes images generated by 8 different state-of-the-art generators, paired with real images from ImageNet.

### Generators Included
1. **Stable Diffusion v1.4**: Latent diffusion model
2. **Stable Diffusion v1.5**: Improved latent diffusion
3. **Midjourney**: High-quality artistic generation
4. **GLIDE**: Guided diffusion
5. **ADM (Guided Diffusion)**: OpenAI's guided diffusion
6. **VQDM**: Vector Quantized Diffusion Model
7. **BigGAN**: Large-scale GAN
8. **Wukong**: Chinese text-to-image model

### Download Instructions (Manual)
Due to size and Google Drive restrictions, this dataset must be downloaded manually.

1. **Visit**: [GenImage Google Drive](https://drive.google.com/drive/folders/1jGt10bwTbhEZuGXLyvrCuxOI0cBqQ1FS)
2. **Download**:
   - `real/` folder
   - `fake/` folder (ensure all 8 subfolders are included)
3. **Extract to**: `data/downloads/genimage/`

**Expected Structure**:
```
data/downloads/genimage/
├── real/
│   └── [image files]
└── fake/
    ├── stable_diffusion_v1.4/
    ├── stable_diffusion_v1.5/
    ├── midjourney/
    ├── glide/
    ├── adm/
    ├── vqdm/
    ├── biggan/
    └── wukong/
```

---

## 2. CIFAKE Dataset

**Source**: [Kaggle - CIFAKE](https://www.kaggle.com/datasets/birdy654/cifake-real-and-ai-generated-synthetic-images)
**Size**: ~3GB
**Images**: 120,000 (60K Real, 60K Fake)

### Description
Contains 60,000 synthetic images generated by Stable Diffusion v1.4 and 60,000 real images from the CIFAR-10 dataset. This dataset helps the model learn to distinguish general objects (airplanes, cars, birds, cats, etc.).

### Download Instructions (Automated)
Run the provided script:
```bash
bash scripts/download_datasets.sh
```
*Requires Kaggle API credentials.*

---

## 3. 140K Real and Fake Faces

**Source**: [Kaggle - 140K Faces](https://www.kaggle.com/datasets/xhlulu/140k-real-and-fake-faces)
**Size**: ~7GB
**Images**: 140,000 (70K Real, 70K Fake)

### Description
Consists of 70k real faces from the Flickr-Faces-HQ (FFHQ) dataset and 70k fake faces generated by StyleGAN. This dataset is crucial for detecting deepfake portraits.

### Download Instructions (Automated)
Run the provided script:
```bash
bash scripts/download_datasets.sh
```
*Requires Kaggle API credentials.*

---

## Organization & Splits

After downloading, run the organization script:
```bash
python src/organize_datasets.py --datasets genimage cifake faces
```

This creates the following splits in `data/`:

### GenImage Splits (70/15/15)
- **Train**: ~280,000 images
- **Val**: ~60,000 images
- **Test**: ~60,000 images

### CIFAKE Splits (70/15/15)
- **Train**: ~84,000 images
- **Val**: ~18,000 images
- **Test**: ~18,000 images

### Faces Splits (60/20/20)
- **Train**: ~84,000 images
- **Val**: ~28,000 images
- **Test**: ~28,000 images

---

## Troubleshooting

### Download Failures
- **GenImage**: If `gdown` fails, use the manual Google Drive link.
- **Kaggle**: Ensure `~/.kaggle/kaggle.json` exists and has correct permissions (`chmod 600`).

### Verification
Run the verification script to check dataset integrity:
```bash
python scripts/verify_datasets.py
```
